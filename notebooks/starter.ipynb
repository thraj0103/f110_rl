{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn Terminal inside venv\\ngit clone https://github.com/f1tenth/f1tenth_gym.git\\ncd f1tenth_gym\\npip install -e .\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install pip==21.0 setuptools==65.5.0\n",
    "# !pip install torch torchvision torchaudio gym==0.19.0 numpy==1.23.5 matplotlib==3.5.3 tqdm \n",
    "\"\"\"\n",
    "In Terminal inside venv\n",
    "git clone https://github.com/f1tenth/f1tenth_gym.git\n",
    "cd f1tenth_gym\n",
    "pip install -e .\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "import f110_gym\n",
    "\n",
    "# Set device (prefer GPU like A100 if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import numba\n",
    "shutil.rmtree(numba.config.CACHE_DIR, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor & Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# PPO with continuous action space uses Gaussian (Normal) policy\n",
    "\n",
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std = nn.Parameter(torch.zeros(act_dim))  # Learnable log std dev\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu_head(x)\n",
    "        std = self.log_std.exp()\n",
    "        return mu, std\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.v_head = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        v = self.v_head(x)\n",
    "        return v\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observation Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def flatten_observation(obs_dict):\n",
    "    \"\"\"\n",
    "    Flatten full LIDAR scan + ego position/heading + velocity\n",
    "    \"\"\"\n",
    "    scan = obs_dict['scans'][0]                 # (1080,)\n",
    "    x = np.array([obs_dict['poses_x'][0]])\n",
    "    y = np.array([obs_dict['poses_y'][0]])\n",
    "    theta = np.array([obs_dict['poses_theta'][0]])\n",
    "    v_x = np.array([obs_dict['linear_vels_x'][0]])\n",
    "    v_y = np.array([obs_dict['linear_vels_y'][0]])\n",
    "\n",
    "    flat_obs = np.concatenate([scan, x, y, theta, v_x, v_y])\n",
    "    flat_obs = np.nan_to_num(flat_obs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    return torch.tensor(flat_obs, dtype=torch.float32).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evn & Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raju0103/Desktop/College Stuff/Northeastern/RL 2/RL_Project/f1tenth_gym/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.\n",
      "  warnings.warn(f\"Chosen integrator is RK4. This is different from previous versions of the gym.\")\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Create the F1TENTH gym environment\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mgym\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mf110_gym:f110-v0\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m               \u001b[49m\u001b[43mnum_agents\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m               \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/Users/raju0103/Desktop/College Stuff/Northeastern/RL 2/RL_Project/map/vegas\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m               \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Reset environment with initial poses\u001b[39;00m\n\u001b[1;32m      8\u001b[0m init_poses \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([[\u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m, \u001b[38;5;241m0.0\u001b[39m]])\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL 2/.venv/lib/python3.9/site-packages/gym/envs/registration.py:184\u001b[0m, in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mmake\u001b[39m(\u001b[38;5;28mid\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mregistry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mid\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL 2/.venv/lib/python3.9/site-packages/gym/envs/registration.py:106\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    104\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking new env: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, path)\n\u001b[1;32m    105\u001b[0m spec \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspec(path)\n\u001b[0;32m--> 106\u001b[0m env \u001b[38;5;241m=\u001b[39m \u001b[43mspec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;66;03m# We used to have people override _reset/_step rather than\u001b[39;00m\n\u001b[1;32m    108\u001b[0m \u001b[38;5;66;03m# reset/step. Set _gym_disable_underscore_compat = True on\u001b[39;00m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;66;03m# your environment if you use these methods and don't want\u001b[39;00m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;66;03m# compatibility code to be invoked.\u001b[39;00m\n\u001b[1;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28mhasattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    113\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(env, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_gym_disable_underscore_compat\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    115\u001b[0m ):\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL 2/.venv/lib/python3.9/site-packages/gym/envs/registration.py:76\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m load(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mentry_point)\n\u001b[0;32m---> 76\u001b[0m     env \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     78\u001b[0m \u001b[38;5;66;03m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[1;32m     79\u001b[0m spec \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mdeepcopy(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL 2/RL_Project/f1tenth_gym/gym/f110_gym/envs/f110_env.py:185\u001b[0m, in \u001b[0;36mF110Env.__init__\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;66;03m# initiate stuff\u001b[39;00m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msim \u001b[38;5;241m=\u001b[39m Simulator(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparams, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_agents, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseed, time_step\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtimestep, integrator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintegrator)\n\u001b[0;32m--> 185\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_ext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    187\u001b[0m \u001b[38;5;66;03m# stateful observations for rendering\u001b[39;00m\n\u001b[1;32m    188\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrender_obs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL 2/RL_Project/f1tenth_gym/gym/f110_gym/envs/base_classes.py:504\u001b[0m, in \u001b[0;36mSimulator.set_map\u001b[0;34m(self, map_path, map_ext)\u001b[0m\n\u001b[1;32m    493\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;124;03mSets the map of the environment and sets the map for scan simulator of each agent\u001b[39;00m\n\u001b[1;32m    495\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    501\u001b[0m \u001b[38;5;124;03m    None\u001b[39;00m\n\u001b[1;32m    502\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m agent \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39magents:\n\u001b[0;32m--> 504\u001b[0m     \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_ext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL 2/RL_Project/f1tenth_gym/gym/f110_gym/envs/base_classes.py:179\u001b[0m, in \u001b[0;36mRaceCar.set_map\u001b[0;34m(self, map_path, map_ext)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mset_map\u001b[39m(\u001b[38;5;28mself\u001b[39m, map_path, map_ext):\n\u001b[1;32m    172\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;124;03m    Sets the map for scan simulator\u001b[39;00m\n\u001b[1;32m    174\u001b[0m \u001b[38;5;124;03m    \u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;124;03m        map_ext (str): extension of the map image file\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m     \u001b[43mRaceCar\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscan_simulator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_map\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_ext\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL 2/RL_Project/f1tenth_gym/gym/f110_gym/envs/laser_models.py:413\u001b[0m, in \u001b[0;36mScanSimulator2D.set_map\u001b[0;34m(self, map_path, map_ext)\u001b[0m\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    412\u001b[0m     map_metadata \u001b[38;5;241m=\u001b[39m yaml\u001b[38;5;241m.\u001b[39msafe_load(yaml_stream)\n\u001b[0;32m--> 413\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmap_resolution \u001b[38;5;241m=\u001b[39m \u001b[43mmap_metadata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mresolution\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[1;32m    414\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morigin \u001b[38;5;241m=\u001b[39m map_metadata[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124morigin\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m yaml\u001b[38;5;241m.\u001b[39mYAMLError \u001b[38;5;28;01mas\u001b[39;00m ex:\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"f110_gym:f110-v0\", \n",
    "               num_agents=1, \n",
    "               map='/Users/raju0103/Desktop/College Stuff/Northeastern/RL 2/RL_Project/map/vegas',\n",
    "               )\n",
    "\n",
    "init_poses = np.array([[0.0, 0.0, 0.0]])\n",
    "obs, _, _, _ = env.reset(poses=init_poses)\n",
    "\n",
    "# Trigger proper action space setup\n",
    "dummy_action = np.array([[0.0, 0.0]])\n",
    "obs, _, done, _ = env.step(dummy_action)\n",
    "\n",
    "# Flatten obs and get dimensions\n",
    "flat_obs = flatten_observation(obs)\n",
    "obs_dim = flat_obs.shape[0]   # should be 1084 now\n",
    "act_dim = 2  # [steering, velocity]\n",
    "\n",
    "# Create models\n",
    "actor_func = ActorNet(obs_dim, act_dim).to(device)\n",
    "value_func = CriticNet(obs_dim).to(device)\n",
    "\n",
    "print(\"Observation dim:\", obs_dim)\n",
    "print(\"Action dim:\", act_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def sample_action_and_logprob(actor, obs_flat):\n",
    "    if torch.isnan(obs_flat).any():\n",
    "        raise ValueError(\"obs_flat contains NaNs\")\n",
    "\n",
    "    mu, std = actor(obs_flat)\n",
    "    if torch.isnan(mu).any() or torch.isnan(std).any():\n",
    "        raise ValueError(f\"Actor output contains NaNs: mu={mu}, std={std}\")\n",
    "\n",
    "    dist = Normal(mu, std)\n",
    "    action = dist.sample()\n",
    "    log_prob = dist.log_prob(action).sum()\n",
    "    entropy = dist.entropy().sum()\n",
    "    return action, log_prob, entropy, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rollout Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "def collect_rollout(env, actor, critic, max_steps=500, render=False):\n",
    "    obs_list, act_list, logprob_list, value_list, reward_list = [], [], [], [], []\n",
    "\n",
    "    init_poses = np.array([[0.0, 0.0, 0.0]])\n",
    "    obs, _, _, _ = env.reset(poses=init_poses)\n",
    "    done = False\n",
    "    steps = 0\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        obs_flat = flatten_observation(obs)\n",
    "        value = critic(obs_flat)\n",
    "        action, log_prob, _, _ = sample_action_and_logprob(actor, obs_flat)\n",
    "\n",
    "        # Save step info\n",
    "        obs_list.append(obs_flat.detach())\n",
    "        act_list.append(action.detach())\n",
    "        logprob_list.append(log_prob.detach())\n",
    "        value_list.append(value.detach())\n",
    "\n",
    "        # Step\n",
    "        action_np = action.cpu().numpy().reshape(1, -1)\n",
    "        obs, _, done, _ = env.step(action_np)\n",
    "\n",
    "        # Reward: encourage forward aligned motion\n",
    "        v_x = obs['linear_vels_x'][0]\n",
    "        v_y = obs['linear_vels_y'][0]\n",
    "        heading = obs['poses_theta'][0]\n",
    "\n",
    "        velocity_mag = np.sqrt(v_x**2 + v_y**2)\n",
    "        velocity_angle = np.arctan2(v_y, v_x)\n",
    "        heading_diff = velocity_angle - heading\n",
    "\n",
    "        # Reward is projection of velocity in heading direction\n",
    "        reward = velocity_mag * np.cos(heading_diff)\n",
    "\n",
    "        # Penalty for early termination (crash or timeout)\n",
    "        if done:\n",
    "            reward -= 5.0\n",
    "\n",
    "        reward_list.append(reward)\n",
    "        steps += 1\n",
    "\n",
    "        if render:\n",
    "            env.render()\n",
    "\n",
    "    return obs_list, act_list, logprob_list, value_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Return & Advantage Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_and_advantages(rewards, values, gamma=0.99, lam=0.95):\n",
    "    \"\"\"\n",
    "    Compute discounted returns and Generalized Advantage Estimates (GAE).\n",
    "    Returns:\n",
    "    - returns: total discounted rewards\n",
    "    - advantages: advantages for each timestep\n",
    "    \"\"\"\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    values = torch.stack(values).squeeze().to(device)\n",
    "    returns = torch.zeros_like(rewards).to(device)\n",
    "    advantages = torch.zeros_like(rewards).to(device)\n",
    "\n",
    "    next_value = 0\n",
    "    next_advantage = 0\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        advantages[t] = delta + gamma * lam * next_advantage\n",
    "        returns[t] = advantages[t] + values[t]\n",
    "\n",
    "        next_value = values[t]\n",
    "        next_advantage = advantages[t]\n",
    "\n",
    "    return returns.detach(), advantages.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Loss and Backward Pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ppo_update(actor, critic, optimizer, obs_batch, act_batch, old_logprobs, returns, advantages,\n",
    "               clip_eps=0.2, vf_coeff=0.5, ent_coeff=0.01):\n",
    "    \"\"\"\n",
    "    Performs a PPO-Clip update step using collected data.\n",
    "    \"\"\"\n",
    "    obs_batch = torch.stack(obs_batch).to(device)\n",
    "    act_batch = torch.stack(act_batch).to(device)\n",
    "    old_logprobs = torch.stack(old_logprobs).to(device)\n",
    "    returns = returns.to(device)\n",
    "    advantages = advantages.to(device)\n",
    "\n",
    "    # Recompute action distribution with current policy\n",
    "    mu, std = actor(obs_batch)\n",
    "    dist = Normal(mu, std)\n",
    "    new_logprobs = dist.log_prob(act_batch).sum(dim=-1)\n",
    "    entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "    # Policy ratio\n",
    "    ratio = torch.exp(new_logprobs - old_logprobs)\n",
    "\n",
    "    # PPO clipped surrogate objective\n",
    "    unclipped = ratio * advantages\n",
    "    clipped = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "    policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "\n",
    "    # Value loss\n",
    "    values = critic(obs_batch).squeeze()\n",
    "    value_loss = F.mse_loss(values, returns)\n",
    "\n",
    "    # Total loss\n",
    "    loss = policy_loss + vf_coeff * value_loss - ent_coeff * entropy.mean()\n",
    "\n",
    "    # Optimize\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item(), entropy.mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Hyperparameters\n",
    "total_episodes = 500\n",
    "gamma = 0.99\n",
    "lam = 0.95\n",
    "clip_eps = 0.2\n",
    "vf_coeff = 0.5\n",
    "ent_coeff = 0.01\n",
    "lr = 1e-4\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.Adam(list(actor_func.parameters()) + list(value_func.parameters()), lr=lr)\n",
    "\n",
    "# Logs\n",
    "reward_records = []\n",
    "loss_records = []\n",
    "\n",
    "# Training loop with tqdm\n",
    "progress_bar = tqdm(range(total_episodes), desc=\"Training PPO\")\n",
    "for episode in progress_bar:\n",
    "    # Collect one rollout\n",
    "    render_flag = (episode % 25 == 0)  # render every 25 episodes\n",
    "\n",
    "    obs_list, act_list, logprob_list, value_list, reward_list = collect_rollout(\n",
    "        env, actor_func, value_func, render=render_flag\n",
    "    )\n",
    "\n",
    "    # Compute returns and advantages\n",
    "    returns, advantages = compute_returns_and_advantages(reward_list, value_list, gamma, lam)\n",
    "\n",
    "    # PPO update\n",
    "    try:\n",
    "        loss, p_loss, v_loss, entropy = ppo_update(\n",
    "            actor_func, value_func, optimizer,\n",
    "            obs_list, act_list, logprob_list,\n",
    "            returns, advantages,\n",
    "            clip_eps=clip_eps, vf_coeff=vf_coeff, ent_coeff=ent_coeff\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Update failed: {e}\")\n",
    "        # Skip this update and continue with training\n",
    "        loss, p_loss, v_loss, entropy = float('nan'), float('nan'), float('nan'), float('nan')\n",
    "\n",
    "    # Logging\n",
    "    ep_reward = sum(reward_list)\n",
    "    reward_records.append(ep_reward)\n",
    "    loss_records.append((loss, p_loss, v_loss, entropy))\n",
    "\n",
    "    # Live metrics in tqdm bar\n",
    "    progress_bar.set_description(\n",
    "        f\"Ep {episode:>3} | R: {ep_reward:>6.1f} | Loss: {loss:.3f} | Ent: {entropy:.2f}\"\n",
    "    )\n",
    "\n",
    "    # Early stopping\n",
    "    if episode >= 50 and np.mean(reward_records[-50:]) > 800:\n",
    "        print(\"\\n✅ Early stopping: average reward > 800.\")\n",
    "        break\n",
    "\n",
    "print(\"✅ Training complete.\")\n",
    "\n",
    "# %%\n",
    "# Save trained actor policy\n",
    "torch.save(actor_func.state_dict(), \"ppo_f110_actor.pt\")\n",
    "print(\"✅ Saved actor weights to 'ppo_f110_actor.pt'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot reward curve and 50-episode moving average\n",
    "def moving_average(data, window_size=50):\n",
    "    if len(data) < window_size:\n",
    "        return data\n",
    "    return np.convolve(data, np.ones(window_size)/window_size, mode='valid')\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(reward_records, label='Episode Reward')\n",
    "plt.plot(moving_average(reward_records), label='Moving Average (50 episodes)', linewidth=2)\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"PPO Training Reward Curve\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
