{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn Terminal inside venv\\ngit clone https://github.com/f1tenth/f1tenth_gym.git\\ncd f1tenth_gym\\npip install -e .\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install pip==21.0 \n",
    "# %pip install setuptools==65.5.0\n",
    "# %pip install torch torchvision torchaudio gym==0.19.0 numpy==1.23.5 matplotlib==3.5.3 tqdm\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\"\n",
    "In Terminal inside venv\n",
    "git clone https://github.com/f1tenth/f1tenth_gym.git\n",
    "cd f1tenth_gym\n",
    "pip install -e .\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set device (prefer GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Define hyperparameters\n",
    "HIDDEN_DIM = 512  # Increased network capacity\n",
    "LEARNING_RATE = 3e-4  # Higher learning rate with scheduling\n",
    "GAMMA = 0.99  # Higher discount factor for long-term planning\n",
    "LAMBDA = 0.95  # GAE parameter\n",
    "CLIP_EPSILON = 0.2  # PPO clipping parameter\n",
    "VF_COEFF = 0.5  # Value function loss coefficient\n",
    "ENT_COEFF = 0.01  # Entropy coefficient\n",
    "PPO_EPOCHS = 10  # More PPO epochs per rollout\n",
    "GRAD_CLIP = 0.5  # Gradient clipping threshold\n",
    "BATCH_SIZE = 64  # Mini-batch size\n",
    "LR_DECAY = 0.9999  # Learning rate decay factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        \n",
    "        # Separate heads for mean and log_std\n",
    "        self.mu_head = nn.Linear(hidden_dim, act_dim)\n",
    "        self.log_std_head = nn.Linear(hidden_dim, act_dim)\n",
    "        \n",
    "        # Initialize weights properly\n",
    "        nn.init.orthogonal_(self.fc1.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.fc2.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.fc3.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.mu_head.weight, gain=0.01)\n",
    "        nn.init.orthogonal_(self.log_std_head.weight, gain=0.01)\n",
    "        \n",
    "        # Action limits for F1TENTH (steering, acceleration)\n",
    "        self.action_bounds = {\n",
    "            'low': torch.tensor([-0.4, 0.0], device=device),\n",
    "            'high': torch.tensor([0.4, 10.0], device=device)\n",
    "        }\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        \n",
    "        mu = self.mu_head(x)\n",
    "        \n",
    "        # Bound mean to valid action range using tanh\n",
    "        mu = torch.tanh(mu)  # Output in [-1, 1]\n",
    "        # Scale to action range\n",
    "        mu = (mu + 1) / 2 * (self.action_bounds['high'] - self.action_bounds['low']) + self.action_bounds['low']\n",
    "        \n",
    "        # State-dependent log_std with constraints\n",
    "        log_std = self.log_std_head(x)\n",
    "        log_std = torch.clamp(log_std, -20, 2)  # Prevent extreme values\n",
    "        std = torch.exp(log_std)\n",
    "        \n",
    "        return mu, std\n",
    "\n",
    "# Improved Critic Network\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "        \n",
    "        # Initialize weights\n",
    "        nn.init.orthogonal_(self.fc1.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.fc2.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.fc3.weight, gain=np.sqrt(2))\n",
    "        nn.init.orthogonal_(self.out.weight, gain=1.0)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Observations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RunningMeanStd:\n",
    "    def __init__(self, shape=(), epsilon=1e-4):\n",
    "        self.mean = np.zeros(shape, dtype=np.float32)\n",
    "        self.var = np.ones(shape, dtype=np.float32)\n",
    "        self.count = epsilon\n",
    "\n",
    "    def update(self, x):\n",
    "        batch_mean = np.mean(x, axis=0)\n",
    "        batch_var = np.var(x, axis=0)\n",
    "        batch_count = x.shape[0]\n",
    "        \n",
    "        delta = batch_mean - self.mean\n",
    "        total_count = self.count + batch_count\n",
    "        \n",
    "        self.mean = self.mean + delta * batch_count / total_count\n",
    "        m_a = self.var * self.count\n",
    "        m_b = batch_var * batch_count\n",
    "        M2 = m_a + m_b + np.square(delta) * self.count * batch_count / total_count\n",
    "        self.var = M2 / total_count\n",
    "        self.count = total_count\n",
    "\n",
    "# Improved observation processor with lap tracking\n",
    "class ObservationProcessor:\n",
    "    def __init__(self, num_beams=270):\n",
    "        self.num_beams = num_beams\n",
    "        self.obs_rms = RunningMeanStd(shape=(num_beams + 5,))\n",
    "        self.is_training = True\n",
    "        \n",
    "        # Track waypoints for lap detection\n",
    "        self.waypoints = []\n",
    "        self.last_waypoint_idx = 0\n",
    "        self.lap_progress = 0.0\n",
    "        self.lap_completed = False\n",
    "        self.previous_progress = 0.0\n",
    "        \n",
    "        # Waypoint collection status\n",
    "        self.waypoints_finalized = False\n",
    "        self.min_laps_for_waypoints = 2\n",
    "        self.waypoint_spacing = 25.0  # Meters between waypoints\n",
    "        \n",
    "        # Lap time tracking\n",
    "        self.current_lap_steps = 0\n",
    "        self.best_lap_steps = float('inf')\n",
    "        self.lap_count = 0\n",
    "        self.current_lap_time = 0.0\n",
    "        self.best_lap_time = float('inf')\n",
    "        \n",
    "        # Start/finish line detection\n",
    "        self.start_line_pos = None\n",
    "        self.lap_almost_complete = False  # Flag to track when we're almost done with a lap\n",
    "        \n",
    "        # Waypoint reward tracking\n",
    "        self.last_reached_waypoint = -1  # Index of the last waypoint reached for incremental rewards\n",
    "        self.waypoint_reached = False    # Flag indicating if a new waypoint was reached in this step\n",
    "        self.closest_waypoint_dist = float('inf')  # Distance to closest waypoint\n",
    "\n",
    "    def process_obs(self, obs, update_stats=True):\n",
    "        \"\"\"Process and normalize the raw observation\"\"\"\n",
    "        # Extract scan data - downsample but keep enough points\n",
    "        scan = obs['scans'][0][::4]  # Downsample LiDAR from 1080 to 270\n",
    "        \n",
    "        # Clip extremely large scan values and replace inf with large value\n",
    "        scan = np.clip(scan, 0.0, 30.0)  \n",
    "        scan[~np.isfinite(scan)] = 30.0\n",
    "        \n",
    "        # Extract other values\n",
    "        x = np.array([obs['poses_x'][0]])\n",
    "        y = np.array([obs['poses_y'][0]])\n",
    "        theta = np.array([obs['poses_theta'][0]])\n",
    "        v_x = np.array([obs['linear_vels_x'][0]])\n",
    "        v_y = np.array([obs['linear_vels_y'][0]])\n",
    "        \n",
    "        # Record position for waypoint collection and lap tracking\n",
    "        pos = (x[0], y[0])\n",
    "        \n",
    "        # Set start line position on first call if not set\n",
    "        if self.start_line_pos is None:\n",
    "            self.start_line_pos = pos\n",
    "        \n",
    "        # Collect waypoints if not finalized\n",
    "        if not self.waypoints_finalized:\n",
    "            self._collect_waypoint(pos)\n",
    "        \n",
    "        # Reset waypoint reached flag\n",
    "        self.waypoint_reached = False\n",
    "        \n",
    "        # Check if we've reached a new waypoint\n",
    "        if len(self.waypoints) > 0:\n",
    "            # Find closest waypoint and distance\n",
    "            closest_idx, closest_dist = self._find_closest_waypoint(pos)\n",
    "            \n",
    "            # Store closest waypoint distance for reward function\n",
    "            self.closest_waypoint_dist = closest_dist\n",
    "            \n",
    "            # Check if we've reached a new waypoint (using index and proximity)\n",
    "            # Don't mark as reached if it's the same as last time\n",
    "            WAYPOINT_REACHED_THRESHOLD = 0.5  # 50cm threshold to consider a waypoint as reached\n",
    "            if closest_dist < WAYPOINT_REACHED_THRESHOLD and closest_idx != self.last_reached_waypoint:\n",
    "                self.waypoint_reached = True\n",
    "                self.last_reached_waypoint = closest_idx\n",
    "        \n",
    "        # Update lap progress and check for completion\n",
    "        if self.waypoints_finalized and len(self.waypoints) > 10:\n",
    "            self._update_lap_progress(pos)\n",
    "        else:\n",
    "            # Simple progress estimation when waypoints aren't available\n",
    "            # Calculate distance from start position to give some sense of progress\n",
    "            dist_from_start = np.sqrt((pos[0] - self.start_line_pos[0])**2 + \n",
    "                                     (pos[1] - self.start_line_pos[1])**2)\n",
    "            \n",
    "            # This is a very rough estimate - higher when far from start\n",
    "            self.lap_progress = min(0.5, dist_from_start / 20.0)  # Assume track length ~40m for scaling\n",
    "        \n",
    "        # Concatenate all observation components\n",
    "        flat_obs = np.concatenate([scan, x, y, theta, v_x, v_y])\n",
    "        \n",
    "        # Update normalization stats during training\n",
    "        if update_stats and self.is_training:\n",
    "            self.obs_rms.update(flat_obs.reshape(1, -1))\n",
    "        \n",
    "        # Normalize the observation\n",
    "        obs_mean = self.obs_rms.mean\n",
    "        obs_var = self.obs_rms.var\n",
    "        normalized_obs = (flat_obs - obs_mean) / np.sqrt(obs_var + 1e-8)\n",
    "        \n",
    "        return torch.tensor(normalized_obs, dtype=torch.float32).to(device)\n",
    "        \n",
    "    def _find_closest_waypoint(self, pos):\n",
    "        \"\"\"Find the closest waypoint and its distance\"\"\"\n",
    "        closest_idx = -1\n",
    "        closest_dist = float('inf')\n",
    "        \n",
    "        for i, waypoint in enumerate(self.waypoints):\n",
    "            dist = np.sqrt((pos[0] - waypoint[0])**2 + (pos[1] - waypoint[1])**2)\n",
    "            if dist < closest_dist:\n",
    "                closest_dist = dist\n",
    "                closest_idx = i\n",
    "                \n",
    "        return closest_idx, closest_dist\n",
    "\n",
    "    def _collect_waypoint(self, pos):\n",
    "        \"\"\"Collect waypoints at regular intervals\"\"\"\n",
    "        if len(self.waypoints) == 0:\n",
    "            self.waypoints.append(pos)\n",
    "            return\n",
    "            \n",
    "        # Check distance to last waypoint\n",
    "        last_waypoint = self.waypoints[-1]\n",
    "        dist = np.sqrt((pos[0] - last_waypoint[0])**2 + (pos[1] - last_waypoint[1])**2)\n",
    "        \n",
    "        # Add waypoint if it's sufficiently distant from the last one\n",
    "        if dist > self.waypoint_spacing:\n",
    "            self.waypoints.append(pos)\n",
    "            \n",
    "        # Check if we've returned close to the first waypoint after collecting many points\n",
    "        # This helps detect when we've come full circle on the track\n",
    "        if len(self.waypoints) > 20:  # Need enough waypoints first\n",
    "            first_waypoint = self.waypoints[0]\n",
    "            dist_to_first = np.sqrt((pos[0] - first_waypoint[0])**2 + (pos[1] - first_waypoint[1])**2)\n",
    "            \n",
    "            # If close to first waypoint and we've moved significantly around track\n",
    "            if dist_to_first < 2.0 and not self.lap_almost_complete:\n",
    "                self.lap_almost_complete = True\n",
    "            elif dist_to_first > 5.0 and self.lap_almost_complete:\n",
    "                # We've moved away from start line after being close to it\n",
    "                self.lap_almost_complete = False\n",
    "\n",
    "    def _update_lap_progress(self, pos):\n",
    "        \"\"\"Update lap progress using collected waypoints\"\"\"\n",
    "        # Find closest waypoint\n",
    "        closest_idx = -1\n",
    "        closest_dist = float('inf')\n",
    "        \n",
    "        for i, waypoint in enumerate(self.waypoints):\n",
    "            dist = np.sqrt((pos[0] - waypoint[0])**2 + (pos[1] - waypoint[1])**2)\n",
    "            if dist < closest_dist:\n",
    "                closest_dist = dist\n",
    "                closest_idx = i\n",
    "        \n",
    "        # Calculate progress (0.0 to 1.0)\n",
    "        total_waypoints = len(self.waypoints)\n",
    "        new_progress = closest_idx / total_waypoints\n",
    "        \n",
    "        # Detect lap completion (crossing from high progress to low progress)\n",
    "        if self.previous_progress > 0.85 and new_progress < 0.15:\n",
    "            # We've wrapped around to the beginning\n",
    "            self.lap_completed = True\n",
    "            self.lap_count += 1\n",
    "            \n",
    "            # Check if this is a faster lap\n",
    "            if self.current_lap_time < self.best_lap_time and self.current_lap_time > 30.0:\n",
    "                self.best_lap_time = self.current_lap_time\n",
    "        else:\n",
    "            self.lap_completed = False\n",
    "        \n",
    "        # Update progress tracking\n",
    "        self.lap_progress = new_progress\n",
    "        self.previous_progress = new_progress\n",
    "        self.current_lap_steps += 1\n",
    "        \n",
    "    def finalize_waypoints(self):\n",
    "        \"\"\"Mark waypoints as finalized and perform cleanup\"\"\"\n",
    "        if not self.waypoints_finalized and len(self.waypoints) > 20:\n",
    "            # Clean up waypoints before finalizing\n",
    "            \n",
    "            # Find closest waypoint to the starting position\n",
    "            start_waypoint_idx = 0\n",
    "            min_dist = float('inf')\n",
    "            \n",
    "            for i, waypoint in enumerate(self.waypoints):\n",
    "                dist = np.sqrt((waypoint[0] - self.start_line_pos[0])**2 + \n",
    "                               (waypoint[1] - self.start_line_pos[1])**2)\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    start_waypoint_idx = i\n",
    "            \n",
    "            # Reorganize waypoints to start from the start line\n",
    "            self.waypoints = self.waypoints[start_waypoint_idx:] + self.waypoints[:start_waypoint_idx]\n",
    "            \n",
    "            # Now mark as finalized\n",
    "            self.waypoints_finalized = True\n",
    "            self.lap_progress = 0.0\n",
    "            self.previous_progress = 0.0\n",
    "            \n",
    "            print(f\"\\nâœ… Waypoints finalized! Collected {len(self.waypoints)} waypoints.\")\n",
    "            return True\n",
    "        return False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Action from Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_action_and_logprob(actor, obs_flat, deterministic=False, training_progress=0.0):\n",
    "    mu, std = actor(obs_flat)\n",
    "    \n",
    "    # Add exploration bias in early training stages\n",
    "    # Gradually add more throttle during initial exploration\n",
    "    if not deterministic and training_progress < 0.2:  # First 20% of training\n",
    "        # Boost the throttle action to encourage faster exploration\n",
    "        exploration_boost = torch.zeros_like(mu)\n",
    "        # Throttle is the second dimension (index 1)\n",
    "        exploration_boost[1] = 0.3 * (1.0 - training_progress * 5)  # Linearly reduce from 0.3 to 0\n",
    "        mu = mu + exploration_boost\n",
    "    \n",
    "    if deterministic:\n",
    "        action = mu\n",
    "        # We'll still compute log_prob for API consistency\n",
    "        dist = Normal(mu, std)\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "    else:\n",
    "        dist = Normal(mu, std)\n",
    "        action = dist.sample()\n",
    "        log_prob = dist.log_prob(action).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "    \n",
    "    # Ensure action is within valid bounds\n",
    "    action = torch.clamp(action, \n",
    "                        min=actor.action_bounds['low'],\n",
    "                        max=actor.action_bounds['high'])\n",
    "    \n",
    "    return action, log_prob, entropy, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_reward(obs, prev_obs, action, step_reward, done, processor):\n",
    "    \"\"\"Calculate reward based on driving performance and lap completion\"\"\"\n",
    "    # Extract key values\n",
    "    # x = obs['poses_x'][0]\n",
    "    # y = obs['poses_y'][0]\n",
    "    # theta = obs['poses_theta'][0]\n",
    "    v_x = obs['linear_vels_x'][0]\n",
    "    v_y = obs['linear_vels_y'][0]\n",
    "    ang_vel_z = obs['ang_vels_z'][0]\n",
    "    steering = action[0, 0]\n",
    "    # throttle = action[0, 1]\n",
    "    min_scan = np.min(obs['scans'][0])\n",
    "    collision = obs['collisions'][0]\n",
    "    \n",
    "    # Speed related reward - encourage higher speeds with stronger incentives\n",
    "    speed = np.hypot(v_x, v_y)\n",
    "    \n",
    "    # More aggressive speed reward structure\n",
    "    speed_reward = 1.0 * speed  # Doubled base speed reward\n",
    "    \n",
    "    # Progressive speed reward that scales up as car goes faster\n",
    "    if speed > 2.0:\n",
    "        speed_reward += 1.0 * (speed - 2.0)  # Additional bonus for speed over 2.0\n",
    "    if speed > 4.0:\n",
    "        speed_reward += 1.0 * (speed - 4.0)  # Even more reward for speeds over 4.0\n",
    "    \n",
    "    # Waypoint-based reward - provide short-term incentives\n",
    "    waypoint_reward = 0.0\n",
    "    \n",
    "    # Give a reward when reaching a new waypoint\n",
    "    if processor.waypoint_reached:\n",
    "        waypoint_reward = 10.0  # Significant reward for reaching a new waypoint\n",
    "    \n",
    "    # Additional distance-based incentive towards closest waypoint\n",
    "    # This creates a continuous gradient to guide the car\n",
    "    WAYPOINT_DISTANCE_SCALE = 5.0  # Increased from 5.0 to compensate for removed progress reward\n",
    "    waypoint_distance_reward = 0.0\n",
    "    \n",
    "    # Only apply if we have waypoints\n",
    "    if len(processor.waypoints) > 0:\n",
    "        # Inverse distance reward (closer = higher reward)\n",
    "        # Clipped to avoid huge rewards when very close\n",
    "        dist = max(0.1, processor.closest_waypoint_dist)  # Prevent division by zero\n",
    "        waypoint_distance_reward = WAYPOINT_DISTANCE_SCALE / dist\n",
    "    \n",
    "    # Penalize going off track or too close to walls\n",
    "    wall_penalty = 0.0\n",
    "    SAFE_DISTANCE = 0.3  # 30cm from walls is danger zone\n",
    "    if min_scan < SAFE_DISTANCE:\n",
    "        # Exponential penalty that increases as car gets closer to walls\n",
    "        wall_penalty = 5.0 * (SAFE_DISTANCE - min_scan) / SAFE_DISTANCE\n",
    "    \n",
    "    # Heavy collision penalty\n",
    "    collision_penalty = 1000.0 if collision == 1 else 0.0\n",
    "    \n",
    "    # Penalize excessive steering for smoother driving\n",
    "    steering_penalty = 0.5 * abs(steering)\n",
    "    \n",
    "    # Penalize excessive angular velocity (spinning)\n",
    "    spin_penalty = 0.5 * abs(ang_vel_z)\n",
    "    \n",
    "    # Smaller step penalty - don't penalize exploration as much\n",
    "    step_penalty = 0.05  # Reduced from 0.1\n",
    "    \n",
    "    # Lap completion rewards\n",
    "    lap_reward = 0.0\n",
    "    \n",
    "    # Check for lap completion via environment's done flag\n",
    "    # If environment says we're done but no collision, we've completed the required laps\n",
    "    if done and collision == 0:\n",
    "        lap_reward = 250.0  # Significant reward for completing required laps\n",
    "        \n",
    "        # Finalize waypoints if we've collected enough laps\n",
    "        if processor.lap_count >= processor.min_laps_for_waypoints and not processor.waypoints_finalized:\n",
    "            processor.finalize_waypoints()\n",
    "    \n",
    "    # Check for lap completion via processor's detection (only if waypoints are finalized)\n",
    "    if processor.waypoints_finalized and processor.lap_completed:\n",
    "        lap_reward += 100.0  # Reward for completing a lap via waypoint detection\n",
    "        \n",
    "        # Bonus for better lap time\n",
    "        if processor.current_lap_time < processor.best_lap_time and processor.current_lap_time > 30.0:\n",
    "            processor.best_lap_time = processor.current_lap_time\n",
    "            lap_reward += 100.0  # Bonus for better lap time\n",
    "            \n",
    "        # Reset lap timer\n",
    "        processor.current_lap_time = 0.0\n",
    "    \n",
    "    # Progress reward based on track position (only if waypoints are finalized)\n",
    "    progress_position_reward = 5.0 * processor.lap_progress if processor.waypoints_finalized else 0.0\n",
    "    \n",
    "    # Calculate total reward\n",
    "    reward = (\n",
    "        speed_reward +\n",
    "        waypoint_reward +\n",
    "        waypoint_distance_reward +\n",
    "        progress_position_reward +\n",
    "        lap_reward -\n",
    "        wall_penalty -\n",
    "        collision_penalty -\n",
    "        steering_penalty -\n",
    "        spin_penalty -\n",
    "        step_penalty\n",
    "    )\n",
    "    \n",
    "    # Clip reward to prevent extreme values\n",
    "    # reward = np.clip(reward, -100.0, 500.0)\n",
    "    \n",
    "    # For debugging\n",
    "    reward_components = {\n",
    "        'speed': speed_reward,\n",
    "        'waypoint': waypoint_reward + waypoint_distance_reward,\n",
    "        'position': progress_position_reward,\n",
    "        'lap': lap_reward,\n",
    "        'wall': -wall_penalty,\n",
    "        'collision': -collision_penalty,\n",
    "        'steering': -steering_penalty,\n",
    "        'spin': -spin_penalty,\n",
    "        'step': -step_penalty,\n",
    "        'total': reward,\n",
    "        'lap_time': processor.current_lap_time\n",
    "    }\n",
    "    \n",
    "    return reward, reward_components\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Rollout with Custom Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollout(env, actor, critic, processor, max_steps=2000, render=False, deterministic=False, episode_num=0, total_episodes=5000):\n",
    "    \"\"\"Collect a training episode with the current policy\"\"\"\n",
    "    obs_list, act_list, logprob_list, value_list, reward_list, reward_components_list = [], [], [], [], [], []\n",
    "    \n",
    "    # Reset environment with fixed starting position\n",
    "    start_x = 0.0\n",
    "    start_y = 0.0\n",
    "    start_theta = -0.6524  # Fixed heading to ensure car starts in the right direction\n",
    "    \n",
    "    obs, step_reward, done, info = env.reset(poses=np.array([[start_x, start_y, start_theta]]))\n",
    "    \n",
    "    # Reset processor lap tracking for this episode\n",
    "    processor.lap_completed = False\n",
    "    processor.current_lap_steps = 0\n",
    "    processor.current_lap_time = 0.0\n",
    "    \n",
    "    # Episode-specific variables\n",
    "    done = False\n",
    "    steps = 0\n",
    "    prev_obs = obs  # Store previous observation for reward calculation\n",
    "    \n",
    "    # Calculate training progress for exploration rate\n",
    "    training_progress = episode_num / total_episodes\n",
    "    \n",
    "    while not done and steps < max_steps:\n",
    "        obs_flat = processor.process_obs(obs)\n",
    "        with torch.no_grad():\n",
    "            value = critic(obs_flat)\n",
    "            action, log_prob, _, _ = sample_action_and_logprob(actor, obs_flat, deterministic, training_progress)\n",
    "        \n",
    "        obs_list.append(obs_flat.detach())\n",
    "        act_list.append(action.detach())\n",
    "        logprob_list.append(log_prob.detach())\n",
    "        value_list.append(value.detach())\n",
    "        \n",
    "        action_np = action.cpu().numpy().reshape(1, -1)\n",
    "        obs, step_reward, done, info = env.step(action_np)\n",
    "        \n",
    "        # Update lap time using step_reward from environment\n",
    "        processor.current_lap_time += step_reward\n",
    "        \n",
    "        # Calculate reward\n",
    "        reward, reward_components = compute_reward(obs, prev_obs, action_np, step_reward, done, processor)\n",
    "        \n",
    "        reward_list.append(reward)\n",
    "        reward_components_list.append(reward_components)\n",
    "        \n",
    "        # Update for next iteration\n",
    "        prev_obs = obs\n",
    "        \n",
    "        steps += 1\n",
    "        \n",
    "        if render:\n",
    "            env.render(mode='human_fast')\n",
    "    \n",
    "    return obs_list, act_list, logprob_list, value_list, reward_list, reward_components_list, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Returns and Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_and_advantages(rewards, values, gamma=0.99, lam=0.95):\n",
    "    \"\"\"Calculate GAE advantages and returns\"\"\"\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    values = torch.cat(values).squeeze().to(device)\n",
    "    \n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "    \n",
    "    next_value = 0\n",
    "    next_advantage = 0\n",
    "    \n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        advantages[t] = delta + gamma * lam * next_advantage\n",
    "        returns[t] = advantages[t] + values[t]\n",
    "        next_value = values[t]\n",
    "        next_advantage = advantages[t]\n",
    "    \n",
    "    # Normalize advantages for more stable learning\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    return returns.detach(), advantages.detach()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Update Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(actor, critic, optimizer, obs_batch, act_batch, old_logprobs, returns, advantages,\n",
    "              clip_eps=0.2, vf_coeff=0.5, ent_coeff=0.01, max_grad_norm=0.5):\n",
    "    \"\"\"Update actor and critic networks using PPO algorithm\"\"\"\n",
    "    obs_batch = torch.stack(obs_batch).to(device)\n",
    "    act_batch = torch.stack(act_batch).to(device)\n",
    "    old_logprobs = torch.stack(old_logprobs).to(device)\n",
    "    returns = returns.to(device)\n",
    "    advantages = advantages.to(device)\n",
    "    \n",
    "    # Calculate batch size based on rollout length\n",
    "    batch_size = len(obs_batch)\n",
    "    minibatch_size = min(64, batch_size)\n",
    "    num_updates = max(10, batch_size // minibatch_size)\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_policy_loss = 0\n",
    "    total_value_loss = 0\n",
    "    total_entropy = 0\n",
    "    \n",
    "    for _ in range(num_updates):\n",
    "        # Randomly sample minibatch\n",
    "        idx = torch.randperm(batch_size)[:minibatch_size]\n",
    "        \n",
    "        mb_obs = obs_batch[idx]\n",
    "        mb_acts = act_batch[idx]\n",
    "        mb_old_logprobs = old_logprobs[idx]\n",
    "        mb_returns = returns[idx]\n",
    "        mb_advantages = advantages[idx]\n",
    "        \n",
    "        # Get current policy distribution\n",
    "        mu, std = actor(mb_obs)\n",
    "        dist = Normal(mu, std)\n",
    "        new_logprobs = dist.log_prob(mb_acts).sum(dim=-1)\n",
    "        entropy = dist.entropy().sum(dim=-1)\n",
    "        \n",
    "        # PPO policy loss\n",
    "        ratio = torch.exp(new_logprobs - mb_old_logprobs)\n",
    "        surr1 = ratio * mb_advantages\n",
    "        surr2 = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * mb_advantages\n",
    "        policy_loss = -torch.min(surr1, surr2).mean()\n",
    "        \n",
    "        # Value function loss\n",
    "        value_pred = critic(mb_obs).squeeze()\n",
    "        value_loss = F.mse_loss(value_pred, mb_returns)\n",
    "        \n",
    "        # Total loss\n",
    "        loss = policy_loss + vf_coeff * value_loss - ent_coeff * entropy.mean()\n",
    "        \n",
    "        # Perform update\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping for stability\n",
    "        nn.utils.clip_grad_norm_(list(actor.parameters()) + list(critic.parameters()), max_grad_norm)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        # Accumulate loss values\n",
    "        total_loss += loss.item()\n",
    "        total_policy_loss += policy_loss.item()\n",
    "        total_value_loss += value_loss.item()\n",
    "        total_entropy += entropy.mean().item()\n",
    "    \n",
    "    # Return average losses\n",
    "    n = num_updates\n",
    "    return total_loss/n, total_policy_loss/n, total_value_loss/n, total_entropy/n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ppo(env_config='Austin_map.yaml', num_episodes=50000, save_interval=25, render_interval=20):\n",
    "    \"\"\"Train the PPO agent on the F1TENTH environment\"\"\"\n",
    "    # Load environment configuration\n",
    "    with open(env_config) as file:\n",
    "        conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    \n",
    "    if conf_dict is None:\n",
    "        raise ValueError(\"âš ï¸ YAML file is empty or malformed!\")\n",
    "    \n",
    "    conf = Namespace(**conf_dict)\n",
    "    \n",
    "    # Create environment\n",
    "    env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "    \n",
    "    # Initialize environment\n",
    "    init_poses = np.array([[0.0, 0.0, 0.0]])\n",
    "    obs, _, _, _ = env.reset(poses=init_poses)\n",
    "    \n",
    "    # Create observation processor\n",
    "    processor = ObservationProcessor()\n",
    "    \n",
    "    # Process observation to get dimensions\n",
    "    flat_obs = processor.process_obs(obs)\n",
    "    obs_dim = flat_obs.shape[0]\n",
    "    act_dim = 2  # Steering and acceleration\n",
    "    \n",
    "    # Create actor and critic networks\n",
    "    actor = ActorNet(obs_dim, act_dim, hidden_dim=256).to(device)\n",
    "    critic = CriticNet(obs_dim, hidden_dim=256).to(device)\n",
    "    \n",
    "    # Create optimizer with learning rate schedule\n",
    "    initial_lr = 3e-4\n",
    "    optimizer = torch.optim.Adam(\n",
    "        list(actor.parameters()) + list(critic.parameters()), \n",
    "        lr=initial_lr\n",
    "    )\n",
    "    \n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = torch.optim.lr_scheduler.LambdaLR(\n",
    "        optimizer, \n",
    "        lambda epoch: max(1.0 - epoch / num_episodes, 0.1)\n",
    "    )\n",
    "    \n",
    "    # Training metrics\n",
    "    reward_records = []\n",
    "    episode_length_records = []\n",
    "    lap_records = []\n",
    "    best_reward = -np.inf\n",
    "    best_lap_time = float('inf')\n",
    "    \n",
    "    print(\"âœ… Starting PPO training...\")\n",
    "    for episode in tqdm(range(num_episodes), desc=\"Training PPO\"):\n",
    "        # Set render flag\n",
    "        render_flag = (episode % render_interval == 0)\n",
    "        \n",
    "        # Collect rollout\n",
    "        rollout = collect_rollout(env, actor, critic, processor, render=render_flag, \n",
    "                                 episode_num=episode, total_episodes=num_episodes)\n",
    "        obs_list, act_list, logprob_list, value_list, reward_list, reward_components_list, steps = rollout\n",
    "        \n",
    "        # Compute returns and advantages\n",
    "        returns, advantages = compute_returns_and_advantages(reward_list, value_list)\n",
    "        \n",
    "        # Update policy\n",
    "        loss, p_loss, v_loss, entropy = ppo_update(\n",
    "            actor, critic, optimizer,\n",
    "            obs_list, act_list, logprob_list,\n",
    "            returns, advantages\n",
    "        )\n",
    "        \n",
    "        # Update learning rate\n",
    "        lr_scheduler.step()\n",
    "        \n",
    "        # Record metrics\n",
    "        total_reward = sum(reward_list)\n",
    "        reward_records.append(total_reward)\n",
    "        episode_length_records.append(steps)\n",
    "        lap_records.append(processor.lap_count)\n",
    "        \n",
    "        # Save best lap time model\n",
    "        if processor.best_lap_time < best_lap_time and processor.best_lap_time < float('inf'):\n",
    "            best_lap_time = processor.best_lap_time\n",
    "            torch.save({\n",
    "                'actor': actor.state_dict(),\n",
    "                'critic': critic.state_dict(),\n",
    "                'obs_rms_mean': processor.obs_rms.mean,\n",
    "                'obs_rms_var': processor.obs_rms.var,\n",
    "                'best_lap_time': best_lap_time,\n",
    "                'waypoints': processor.waypoints,\n",
    "                'waypoints_finalized': processor.waypoints_finalized,\n",
    "            }, \"best_laptime_model.pt\")\n",
    "            print(f\"\\nðŸ† New best lap time: {best_lap_time:.2f} seconds! Model saved.\")\n",
    "        \n",
    "        # Save best reward model\n",
    "        if total_reward > best_reward:\n",
    "            best_reward = total_reward\n",
    "            torch.save({\n",
    "                'actor': actor.state_dict(),\n",
    "                'critic': critic.state_dict(),\n",
    "                'obs_rms_mean': processor.obs_rms.mean,\n",
    "                'obs_rms_var': processor.obs_rms.var,\n",
    "                'best_reward': best_reward,\n",
    "                'waypoints': processor.waypoints,\n",
    "                'waypoints_finalized': processor.waypoints_finalized,\n",
    "            }, \"best_reward_model.pt\")\n",
    "        \n",
    "        # Save checkpoint periodically\n",
    "        # if episode % save_interval == 0:\n",
    "        #     torch.save({\n",
    "        #         'actor': actor.state_dict(),\n",
    "        #         'critic': critic.state_dict(),\n",
    "        #         'obs_rms_mean': processor.obs_rms.mean,\n",
    "        #         'obs_rms_var': processor.obs_rms.var,\n",
    "        #         'optimizer': optimizer.state_dict(),\n",
    "        #         'episode': episode,\n",
    "        #         'reward_records': reward_records,\n",
    "        #         'lap_records': lap_records,\n",
    "        #         'waypoints': processor.waypoints,\n",
    "        #         'waypoints_finalized': processor.waypoints_finalized,\n",
    "        #     }, f\"ppo_checkpoint_{episode}.pt\")\n",
    "        \n",
    "        # Print detailed info periodically\n",
    "        if episode % 50 == 0:\n",
    "            avg_reward = np.mean(reward_records[-10:]) if len(reward_records) >= 10 else np.mean(reward_records)\n",
    "            avg_steps = np.mean(episode_length_records[-10:]) if len(episode_length_records) >= 10 else np.mean(episode_length_records)\n",
    "            \n",
    "            # Print status update\n",
    "            # print(f\"\\nEpisode {episode}: Reward={total_reward:.2f}, Steps={steps}, Laps={processor.lap_count}\")\n",
    "            # print(f\"Waypoints: {len(processor.waypoints)} collected, Finalized: {processor.waypoints_finalized}\")\n",
    "            # print(f\"Avg Reward (10): {avg_reward:.2f}, Avg Steps: {avg_steps:.1f}\")\n",
    "            \n",
    "            if processor.best_lap_time < float('inf'):\n",
    "                print(f\"Best lap time: {processor.best_lap_time:.2f} seconds\")\n",
    "            \n",
    "            if reward_components_list:\n",
    "                # Show average reward components\n",
    "                components = {k: 0 for k in reward_components_list[0].keys()}\n",
    "                for comp in reward_components_list:\n",
    "                    for k, v in comp.items():\n",
    "                        components[k] += v\n",
    "                n = len(reward_components_list)\n",
    "                for k, v in components.items():\n",
    "                    components[k] = v / n\n",
    "                \n",
    "                # print(\"Avg Reward Components:\")\n",
    "                # print(f\"  Speed: {components['speed']:.2f}\")\n",
    "                # print(f\"  Waypoint: {components['waypoint']:.2f}\")\n",
    "                # print(f\"  Position: {components['position']:.2f}\")\n",
    "                # print(f\"  Lap: {components['lap']:.2f}\")\n",
    "                # print(f\"  Collision: {components['collision']:.2f}\")\n",
    "                # print(f\"  Wall: {components['wall']:.2f}\")\n",
    "    \n",
    "    # Plot training curves and statistics\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(reward_records)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Total Reward\")\n",
    "    plt.title(\"Training Reward Curve\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(episode_length_records)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Episode Length\")\n",
    "    plt.title(\"Episode Length\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    window_size = 10\n",
    "    smoothed_rewards = [np.mean(reward_records[max(0, i-window_size):i+1]) for i in range(len(reward_records))]\n",
    "    \n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(smoothed_rewards)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Smoothed Reward (Window=10)\")\n",
    "    plt.title(\"Smoothed Reward Curve\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(lap_records)\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Laps Completed\")\n",
    "    plt.title(\"Laps Completed\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Add additional plots for debugging\n",
    "    # Plot the last few episodes' rewards more closely\n",
    "    last_n = min(50, len(reward_records))\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(range(len(reward_records)-last_n, len(reward_records)), reward_records[-last_n:])\n",
    "    plt.xlabel(\"Episode\")\n",
    "    plt.ylabel(\"Reward\")\n",
    "    plt.title(f\"Last {last_n} Episodes Reward\")\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot lap completion rate if any laps completed\n",
    "    if max(lap_records) > 0:\n",
    "        plt.subplot(3, 2, 6)\n",
    "        lap_differences = [lap_records[i] - lap_records[i-1] if i > 0 else lap_records[i] for i in range(len(lap_records))]\n",
    "        window_size = min(10, len(lap_differences))\n",
    "        smoothed_lap_rate = [np.mean(lap_differences[max(0, i-window_size):i+1]) for i in range(len(lap_differences))]\n",
    "        plt.plot(smoothed_lap_rate)\n",
    "        plt.xlabel(\"Episode\")\n",
    "        plt.ylabel(\"Lap Completion Rate\")\n",
    "        plt.title(\"Smoothed Lap Completion Rate\")\n",
    "        plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"training_curves.png\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Print additional summary statistics \n",
    "    print(\"\\n===== Training Summary =====\")\n",
    "    print(f\"Total Episodes: {num_episodes}\")\n",
    "    print(f\"Best Reward: {best_reward:.2f}\")\n",
    "    if best_lap_time < float('inf'):\n",
    "        print(f\"Best Lap Time: {best_lap_time:.2f} seconds\")\n",
    "    print(f\"Total Laps Completed: {processor.lap_count}\")\n",
    "    print(f\"Waypoints Collected: {len(processor.waypoints)}\")\n",
    "    print(f\"Waypoints Finalized: {processor.waypoints_finalized}\")\n",
    "    \n",
    "    return actor, critic, processor\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy(env, actor, processor, max_steps=1000):\n",
    "    \"\"\"Render the trained policy and evaluate performance\"\"\"\n",
    "    # Switch processor to evaluation mode\n",
    "    processor.is_training = False\n",
    "    \n",
    "    # Reset environment\n",
    "    obs, step_reward, done, info = env.reset(poses=np.array([[0.0, 0.0, 0.0]]))\n",
    "    \n",
    "    # Reset lap tracking\n",
    "    processor.lap_completed = False\n",
    "    processor.current_lap_time = 0.0\n",
    "    \n",
    "    total_reward = 0\n",
    "    step = 0\n",
    "    lap_times = []\n",
    "    \n",
    "    print(\"\\nðŸ Starting evaluation run...\")\n",
    "    \n",
    "    while not done and step < max_steps:\n",
    "        # Process observation\n",
    "        obs_flat = processor.process_obs(obs, update_stats=False)\n",
    "        \n",
    "        # Get action from policy (deterministic)\n",
    "        with torch.no_grad():\n",
    "            action, _, _, _ = sample_action_and_logprob(actor, obs_flat, deterministic=True)\n",
    "        \n",
    "        # Take step in environment\n",
    "        action_np = action.cpu().numpy().reshape(1, -1)\n",
    "        next_obs, step_reward, done, info = env.step(action_np)\n",
    "        \n",
    "        # Update lap time\n",
    "        processor.current_lap_time += step_reward\n",
    "        \n",
    "        # Calculate reward (for display only)\n",
    "        reward, components = compute_reward(next_obs, obs, action_np, step_reward, done, processor)\n",
    "        total_reward += reward\n",
    "        \n",
    "        # Check if lap was completed this step\n",
    "        if processor.lap_completed:\n",
    "            lap_times.append(processor.current_lap_time)\n",
    "            print(f\"ðŸŽï¸ Lap {len(lap_times)} completed in {processor.current_lap_time:.2f} seconds\")\n",
    "            processor.current_lap_time = 0.0\n",
    "        \n",
    "        # Update for next iteration\n",
    "        obs = next_obs\n",
    "        step += 1\n",
    "        \n",
    "        # Print occasional status\n",
    "        # if step % 100 == 0:\n",
    "        #     speed = np.hypot(obs['linear_vels_x'][0], obs['linear_vels_y'][0])\n",
    "        #     print(f\"Step {step}: Speed = {speed:.2f} m/s, Progress = {processor.lap_progress:.2f}\")\n",
    "        \n",
    "        # Render environment\n",
    "        env.render()\n",
    "        time.sleep(0.1)  # Slow down rendering for better visualization\n",
    "    \n",
    "    # Summary\n",
    "    print(f\"\\nâœ… Evaluation completed after {step} steps with total reward {total_reward:.2f}\")\n",
    "    print(f\"Completed {processor.lap_count} laps\")\n",
    "    \n",
    "    if lap_times:\n",
    "        print(\"\\nLap times:\")\n",
    "        for i, time in enumerate(lap_times):\n",
    "            print(f\"  Lap {i+1}: {time:.2f} seconds\")\n",
    "        print(f\"  Best lap: {min(lap_times):.2f} seconds\")\n",
    "    \n",
    "    # Switch processor back to training mode\n",
    "    processor.is_training = True\n",
    "    \n",
    "    return lap_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(checkpoint_path, obs_dim, act_dim):\n",
    "    \"\"\"Load a trained model from checkpoint file\"\"\"\n",
    "    checkpoint = torch.load(checkpoint_path)\n",
    "    \n",
    "    actor = ActorNet(obs_dim, act_dim).to(device)\n",
    "    critic = CriticNet(obs_dim).to(device)\n",
    "    \n",
    "    actor.load_state_dict(checkpoint['actor'])\n",
    "    critic.load_state_dict(checkpoint['critic'])\n",
    "    \n",
    "    # Create processor with saved normalization parameters\n",
    "    processor = ObservationProcessor()\n",
    "    processor.obs_rms.mean = checkpoint['obs_rms_mean']\n",
    "    processor.obs_rms.var = checkpoint['obs_rms_var']\n",
    "    \n",
    "    # Load waypoints if available\n",
    "    if 'waypoints' in checkpoint:\n",
    "        processor.waypoints = checkpoint['waypoints']\n",
    "    if 'waypoints_finalized' in checkpoint:\n",
    "        processor.waypoints_finalized = checkpoint['waypoints_finalized']\n",
    "    \n",
    "    print(f\"Loaded model from {checkpoint_path}\")\n",
    "    print(f\"Waypoints: {len(processor.waypoints)} loaded, Finalized: {processor.waypoints_finalized}\")\n",
    "    \n",
    "    if 'best_lap_time' in checkpoint:\n",
    "        print(f\"Best lap time in checkpoint: {checkpoint['best_lap_time']:.2f} seconds\")\n",
    "    if 'best_reward' in checkpoint:\n",
    "        print(f\"Best reward in checkpoint: {checkpoint['best_reward']:.2f}\")\n",
    "    \n",
    "    return actor, critic, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training new model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raju0103/Desktop/College Stuff/Northeastern/RL/f110_rl/f1tenth_gym/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.\n",
      "  warnings.warn(f\"Chosen integrator is RK4. This is different from previous versions of the gym.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Starting PPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO:   0%|          | 0/100000 [00:00<?, ?it/s]2025-04-09 02:01:59.736 Python[61464:11808343] ApplePersistenceIgnoreState: Existing state will not be touched. New state will be written to /var/folders/8j/tyq2b18j2w320hyzqh054kr80000gn/T/org.python.python.savedState\n",
      "2025-04-09 02:02:00.476 Python[61464:11808343] +[IMKClient subclass]: chose IMKClient_Modern\n",
      "2025-04-09 02:02:00.592 Python[61464:11808343] +[IMKInputSession subclass]: chose IMKInputSession_Modern\n",
      "Training PPO:   0%|          | 385/100000 [00:34<3:15:01,  8.51it/s]"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Command line arguments could be added here\n",
    "    \n",
    "    # Set random seeds for reproducibility\n",
    "    torch.manual_seed(42)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Train or load model\n",
    "    TRAIN_NEW_MODEL = True  # Set to False to load a saved model\n",
    "    \n",
    "    if TRAIN_NEW_MODEL:\n",
    "        print(\"Training new model...\")\n",
    "        actor, critic, processor = train_ppo(num_episodes=100000)\n",
    "    else:\n",
    "        # Load environment to get observation dimensions\n",
    "        with open('Austin_map.yaml') as file:\n",
    "            conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "        conf = Namespace(**conf_dict)\n",
    "        env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "        obs, _, _, _ = env.reset(poses=np.array([[0.0, 0.0, 0.0]]))\n",
    "        \n",
    "        # Create processor to get observation dimensions\n",
    "        temp_processor = ObservationProcessor()\n",
    "        flat_obs = temp_processor.process_obs(obs)\n",
    "        obs_dim = flat_obs.shape[0]\n",
    "        act_dim = 2\n",
    "        \n",
    "        # Load model\n",
    "        model_path = \"best_laptime_model.pt\"  # Change to desired model\n",
    "        actor, critic, processor = load_model(model_path, obs_dim, act_dim)\n",
    "    \n",
    "    # Create environment for rendering\n",
    "    with open('Austin_map.yaml') as file:\n",
    "        conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    conf = Namespace(**conf_dict)\n",
    "    env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "    \n",
    "    # Render and evaluate trained policy\n",
    "    lap_times = render_policy(env, actor, processor)\n",
    "    \n",
    "    print(\"âœ… Evaluation complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
