{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nIn Terminal inside venv\\ngit clone https://github.com/f1tenth/f1tenth_gym.git\\ncd f1tenth_gym\\npip install -e .\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# %pip install pip==21.0 setuptools==65.5.0\n",
    "# %pip install torch torchvision torchaudio gym==0.19.0 numpy==1.23.5 matplotlib==3.5.3 tqdm\n",
    "# %pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "\"\"\"\n",
    "In Terminal inside venv\n",
    "git clone https://github.com/f1tenth/f1tenth_gym.git\n",
    "cd f1tenth_gym\n",
    "pip install -e .\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.distributions import Normal\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import yaml\n",
    "from argparse import Namespace\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "# Set device (prefer GPU if available)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)\n",
    "\n",
    "# Define hyperparameters\n",
    "HIDDEN_DIM = 512  # Increased network capacity\n",
    "LEARNING_RATE = 3e-4  # Higher learning rate with scheduling\n",
    "GAMMA = 0.99  # Higher discount factor for long-term planning\n",
    "LAMBDA = 0.95  # GAE parameter\n",
    "CLIP_EPSILON = 0.2  # PPO clipping parameter\n",
    "VF_COEFF = 0.5  # Value function loss coefficient\n",
    "ENT_COEFF = 0.01  # Entropy coefficient\n",
    "PPO_EPOCHS = 10  # More PPO epochs per rollout\n",
    "GRAD_CLIP = 0.5  # Gradient clipping threshold\n",
    "BATCH_SIZE = 64  # Mini-batch size\n",
    "LR_DECAY = 0.9999  # Learning rate decay factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Actor and Critic Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorNet(nn.Module):\n",
    "    def __init__(self, obs_dim, act_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mu_head = nn.Linear(hidden_dim, act_dim)\n",
    "\n",
    "        # Learnable log_std initialized to safe value\n",
    "        self.log_std_param = nn.Parameter(torch.full((act_dim,), -0.05))\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = torch.clamp(x, -1e6, 1e6)  # Clamp obs input\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        mu = self.mu_head(x)\n",
    "\n",
    "        # Clamp log_std to avoid NaN in exp\n",
    "        log_std = self.log_std_param\n",
    "        std = torch.exp(log_std)\n",
    "\n",
    "        # Final safety check\n",
    "        if torch.isnan(mu).any() or torch.isnan(std).any():\n",
    "            raise ValueError(f\"NaNs in actor output: mu={mu}, std={std}\")\n",
    "\n",
    "        return mu, std\n",
    "\n",
    "\n",
    "class CriticNet(nn.Module):\n",
    "    def __init__(self, obs_dim, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(obs_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.out(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flatten Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_observation(obs):\n",
    "    scan = obs['scans'][0][::4]  # from 1080 → 270          \n",
    "    x = np.array([obs['poses_x'][0]])\n",
    "    y = np.array([obs['poses_y'][0]])\n",
    "    theta = np.array([obs['poses_theta'][0]])\n",
    "    v_x = np.array([obs['linear_vels_x'][0]])\n",
    "    v_y = np.array([obs['linear_vels_y'][0]])\n",
    "\n",
    "    flat_obs = np.concatenate([scan, x, y, theta, v_x, v_y])\n",
    "    # flat_obs = np.nan_to_num(flat_obs, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "    # flat_obs = np.clip(flat_obs, -1e6, 1e6)  # <-- Add this line\n",
    "    return torch.tensor(flat_obs, dtype=torch.float32).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Action from Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.distributions import Normal\n",
    "\n",
    "def sample_action_and_logprob(actor, obs_flat, episode_num=None):\n",
    "    mu, std = actor(obs_flat)\n",
    "    dist = Normal(mu, std)\n",
    "    action = dist.sample()\n",
    "\n",
    "    # Add extra exploration noise in early episodes\n",
    "    if episode_num is not None and episode_num < 1000:\n",
    "        action += torch.randn_like(action) * 0.1  # or 0.1\n",
    "\n",
    "    log_prob = dist.log_prob(action).sum()\n",
    "    entropy = dist.entropy().sum()\n",
    "\n",
    "    # action = torch.clamp(action,\n",
    "    #                      min=torch.tensor([-0.5, 0.0], device=action.device),\n",
    "    #                      max=torch.tensor([ 0.5, 10.0], device=action.device))\n",
    "    return action, log_prob, entropy, dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Rollout with Custom Reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_rollout(env, actor, critic, max_steps=10000, render=False):\n",
    "    obs_list, act_list, logprob_list, value_list, reward_list = [], [], [], [], []\n",
    "    obs, _, _, _ = env.reset(poses=np.array([[0.0, 0.0, -0.6524]]))\n",
    "    done = False\n",
    "    steps = 0\n",
    "    prev_x = obs['poses_x'][0]\n",
    "    prev_y = obs['poses_y'][0]\n",
    "\n",
    "    while not done and steps < max_steps:\n",
    "        obs_flat = flatten_observation(obs)\n",
    "        value = critic(obs_flat)\n",
    "        action, log_prob, _, _ = sample_action_and_logprob(actor, obs_flat, steps)\n",
    "\n",
    "        obs_list.append(obs_flat.detach())\n",
    "        act_list.append(action.detach())\n",
    "        logprob_list.append(log_prob.detach())\n",
    "        value_list.append(value.detach())\n",
    "\n",
    "        action_np = action.cpu().numpy().reshape(1, -1)\n",
    "        obs, _, done, _ = env.step(action_np)\n",
    "\n",
    "        # Extract key values\n",
    "        v_x = obs['linear_vels_x'][0]\n",
    "        v_y = obs['linear_vels_y'][0]\n",
    "        curr_x = obs['poses_x'][0]\n",
    "        curr_y = obs['poses_y'][0]\n",
    "        heading = obs['poses_theta'][0]\n",
    "        ang_vel_z = obs['ang_vels_z'][0]\n",
    "        steering = action_np[0, 0]\n",
    "        min_scan = np.min(obs['scans'][0])\n",
    "        collision = obs['collisions'][0]\n",
    "\n",
    "        # Motion analysis\n",
    "        delta_x = curr_x - prev_x\n",
    "        delta_y = curr_y - prev_y\n",
    "        velocity_mag = np.hypot(v_x, v_y)\n",
    "        velocity_angle = np.arctan2(v_y, v_x)\n",
    "        heading_diff = (velocity_angle - heading + np.pi) % (2 * np.pi) - np.pi\n",
    "        progress = delta_x * np.cos(heading) + delta_y * np.sin(heading)\n",
    "\n",
    "        # # Reward components\n",
    "        # r_progress = 6.0 * max(0.0, progress)\n",
    "        r_distance = 200.0 * (1 - np.exp(-50.0 * progress))\n",
    "        r_velocity = 200.0 * (1 / (1 + np.exp(-5 * (velocity_mag - 0.1))) - 0.5)\n",
    "        r_spin = -5 * abs(ang_vel_z)\n",
    "        r_steering = -0.5 * abs(steering) * velocity_mag\n",
    "        r_heading = -0.3 * abs(heading_diff)\n",
    "        r_collision = -100.0 if collision == 1 else 0.0\n",
    "        r_wall = -5.0 if min_scan < 0.05 else 0.0\n",
    "        r_step = -0.05\n",
    "\n",
    "        reward = (\n",
    "            r_distance +\n",
    "            r_velocity +\n",
    "            r_spin +\n",
    "            r_heading +\n",
    "            r_steering +\n",
    "            r_collision +\n",
    "            r_wall +\n",
    "            r_step\n",
    "        )\n",
    "\n",
    "        reward_list.append(reward)\n",
    "        steps += 1\n",
    "        prev_x, prev_y = curr_x, curr_y\n",
    "\n",
    "        # # 👀 Debug output for first 10 steps\n",
    "        # if steps >= 500 and steps <= 510:\n",
    "        #     print(f\"\\n🧾 Step {steps} reward breakdown:\")\n",
    "        #     print(f\"  Progress:      {r_distance:+.3f}\")\n",
    "        #     print(f\"  Velocity:      {r_velocity:+.3f}\")\n",
    "        #     print(f\"  Spinning:      {r_spin:+.3f}\")\n",
    "        #     print(f\"  Steering:      {r_steering:+.3f}\")\n",
    "        #     # print(f\"  Heading diff:  {r_heading:+.3f}\")\n",
    "        #     print(f\"  Wall warning:  {r_wall:+.3f}\")\n",
    "        #     print(f\"  Collision:     {r_collision:+.3f}\")\n",
    "        #     print(f\"  Step penalty:  {r_step:+.3f}\")\n",
    "        #     print(f\"  👉 Total:       {reward:+.3f}\")\n",
    "\n",
    "        if render:\n",
    "            env.render(mode='human_fast')\n",
    "\n",
    "    return obs_list, act_list, logprob_list, value_list, reward_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Returns and Advantages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_returns_and_advantages(rewards, values, gamma=0.98, lam=0.95):\n",
    "    rewards = torch.tensor(rewards, dtype=torch.float32).to(device)\n",
    "    values = torch.stack(values).squeeze().to(device)\n",
    "    returns = torch.zeros_like(rewards)\n",
    "    advantages = torch.zeros_like(rewards)\n",
    "\n",
    "    next_value = 0\n",
    "    next_advantage = 0\n",
    "\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        delta = rewards[t] + gamma * next_value - values[t]\n",
    "        advantages[t] = delta + gamma * lam * next_advantage\n",
    "        returns[t] = advantages[t] + values[t]\n",
    "        next_value = values[t]\n",
    "        next_advantage = advantages[t]\n",
    "\n",
    "    return returns.detach(), advantages.detach()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO Update Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppo_update(actor, critic, optimizer, obs_batch, act_batch, old_logprobs, returns, advantages,\n",
    "               clip_eps=0.2, vf_coeff=0.5, ent_coeff=0.1):\n",
    "    obs_batch = torch.stack(obs_batch).to(device)\n",
    "    act_batch = torch.stack(act_batch).to(device)\n",
    "    old_logprobs = torch.stack(old_logprobs).to(device)\n",
    "    returns = returns.to(device)\n",
    "    advantages = advantages.to(device)\n",
    "\n",
    "    mu, std = actor(obs_batch)\n",
    "    dist = Normal(mu, std)\n",
    "    new_logprobs = dist.log_prob(act_batch).sum(dim=-1)\n",
    "    entropy = dist.entropy().sum(dim=-1)\n",
    "\n",
    "    ratio = torch.exp(new_logprobs - old_logprobs)\n",
    "    unclipped = ratio * advantages\n",
    "    clipped = torch.clamp(ratio, 1 - clip_eps, 1 + clip_eps) * advantages\n",
    "    policy_loss = -torch.min(unclipped, clipped).mean()\n",
    "    value_loss = F.mse_loss(critic(obs_batch).squeeze(), returns)\n",
    "    loss = policy_loss + vf_coeff * value_loss - ent_coeff * entropy.mean()\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss.item(), policy_loss.item(), value_loss.item(), entropy.mean().item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/raju0103/Desktop/College Stuff/Northeastern/RL/f110_rl/f1tenth_gym/gym/f110_gym/envs/base_classes.py:93: UserWarning: Chosen integrator is RK4. This is different from previous versions of the gym.\n",
      "  warnings.warn(f\"Chosen integrator is RK4. This is different from previous versions of the gym.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 3: Resetting env...\n",
      "Step 4: Env reset done.\n",
      "✅ Starting PPO training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training PPO:   1%|▏         | 1495/100000 [29:19<32:11:45,  1.18s/it] \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "NaNs in actor output: mu=tensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<AddmmBackward0>), std=tensor([nan, nan], grad_fn=<ExpBackward0>)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 33\u001b[0m\n\u001b[1;32m     30\u001b[0m returns, advantages \u001b[38;5;241m=\u001b[39m compute_returns_and_advantages(reward_list, value_list)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m8\u001b[39m):  \u001b[38;5;66;03m# PPO epochs per rollout\u001b[39;00m\n\u001b[0;32m---> 33\u001b[0m     loss, p_loss, v_loss, entropy \u001b[38;5;241m=\u001b[39m \u001b[43mppo_update\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     34\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactor_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m        \u001b[49m\u001b[43mobs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mact_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogprob_list\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madvantages\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m total_reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(reward_list)\n\u001b[1;32m     40\u001b[0m reward_records\u001b[38;5;241m.\u001b[39mappend(total_reward)\n",
      "Cell \u001b[0;32mIn[17], line 9\u001b[0m, in \u001b[0;36mppo_update\u001b[0;34m(actor, critic, optimizer, obs_batch, act_batch, old_logprobs, returns, advantages, clip_eps, vf_coeff, ent_coeff)\u001b[0m\n\u001b[1;32m      6\u001b[0m returns \u001b[38;5;241m=\u001b[39m returns\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m      7\u001b[0m advantages \u001b[38;5;241m=\u001b[39m advantages\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m----> 9\u001b[0m mu, std \u001b[38;5;241m=\u001b[39m \u001b[43mactor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobs_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m dist \u001b[38;5;241m=\u001b[39m Normal(mu, std)\n\u001b[1;32m     11\u001b[0m new_logprobs \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39mlog_prob(act_batch)\u001b[38;5;241m.\u001b[39msum(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL/f110_rl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/College Stuff/Northeastern/RL/f110_rl/.venv/lib/python3.9/site-packages/torch/nn/modules/module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[0;32mIn[12], line 23\u001b[0m, in \u001b[0;36mActorNet.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# Final safety check\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(mu)\u001b[38;5;241m.\u001b[39many() \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misnan(std)\u001b[38;5;241m.\u001b[39many():\n\u001b[0;32m---> 23\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNaNs in actor output: mu=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmu\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, std=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstd\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mu, std\n",
      "\u001b[0;31mValueError\u001b[0m: NaNs in actor output: mu=tensor([[nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan],\n        [nan, nan]], grad_fn=<AddmmBackward0>), std=tensor([nan, nan], grad_fn=<ExpBackward0>)"
     ]
    }
   ],
   "source": [
    "with open('Austin_map.yaml') as file:\n",
    "        conf_dict = yaml.load(file, Loader=yaml.FullLoader)\n",
    "\n",
    "if conf_dict is None:\n",
    "    raise ValueError(\"⚠️ YAML file is empty or malformed!\")\n",
    "\n",
    "conf = Namespace(**conf_dict)\n",
    "\n",
    "env = gym.make('f110_gym:f110-v0', map=conf.map_path, map_ext=conf.map_ext, num_agents=1)\n",
    "\n",
    "init_poses = np.array([[0.0, 0.0, 0.0]])\n",
    "print(\"Step 3: Resetting env...\")\n",
    "obs, _, _, _ = env.reset(poses=init_poses)\n",
    "print(\"Step 4: Env reset done.\")\n",
    "flat_obs = flatten_observation(obs)\n",
    "obs_dim = flat_obs.shape[0]\n",
    "act_dim = 2\n",
    "\n",
    "actor_func = ActorNet(obs_dim, act_dim).to(device)\n",
    "value_func = CriticNet(obs_dim).to(device)\n",
    "optimizer = torch.optim.Adam(list(actor_func.parameters()) + list(value_func.parameters()), lr=2e-5)\n",
    "\n",
    "reward_records = []\n",
    "best_reward = -np.inf\n",
    "print(\"✅ Starting PPO training...\")\n",
    "for episode in tqdm(range(100000), desc=\"Training PPO\"):\n",
    "    render_flag = (episode % 25 == 0)\n",
    "    rollout = collect_rollout(env, actor_func, value_func, render=render_flag)\n",
    "    obs_list, act_list, logprob_list, value_list, reward_list = rollout\n",
    "    returns, advantages = compute_returns_and_advantages(reward_list, value_list)\n",
    "\n",
    "    for _ in range(8):  # PPO epochs per rollout\n",
    "        loss, p_loss, v_loss, entropy = ppo_update(\n",
    "            actor_func, value_func, optimizer,\n",
    "            obs_list, act_list, logprob_list,\n",
    "            returns, advantages\n",
    "        )\n",
    "\n",
    "    total_reward = sum(reward_list)\n",
    "    reward_records.append(total_reward)\n",
    "\n",
    "    if total_reward > best_reward:\n",
    "        best_reward = total_reward\n",
    "        torch.save(actor_func.state_dict(), \"best_ppo_actor.pt\")\n",
    "\n",
    "    # print(f\"Episode {episode+1}: Total Reward = {total_reward:.2f}, Best = {best_reward:.2f}\")\n",
    "\n",
    "    # if episode >= 100 and np.mean(reward_records[-100:]) > 1000:\n",
    "    #     print(\"✅ Early stopping: stable policy learned\")\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot Rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(reward_records, label=\"Reward\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Total Reward\")\n",
    "plt.title(\"PPO Training Reward Curve\")\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Render Trained Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_policy(env, actor, max_steps=500):\n",
    "    obs, _, _, _ = env.reset(poses=np.array([[0.0, 0.0, 0.0]]))\n",
    "    done = False\n",
    "    for _ in range(max_steps):\n",
    "        obs_flat = flatten_observation(obs)\n",
    "        with torch.no_grad():\n",
    "            action, _, _, _ = sample_action_and_logprob(actor, obs_flat)\n",
    "        action_np = action.cpu().numpy().reshape(1, -1)\n",
    "        obs, _, done, _ = env.step(action_np)\n",
    "        env.render()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "render_policy(env, actor_func)\n",
    "print(\"✅ Render complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
